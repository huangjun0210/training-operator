#!/usr/bin/env python
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Top-level script for running examples."""

from __future__ import print_function

import argparse
import datetime
import logging
import os
import pprint
import StringIO
import subprocess
import time
import urllib

import jinja2
import kubernetes
import yaml
from googleapiclient import discovery
from jinja2 import Environment
from kubernetes import client as k8s_client
from kubernetes import config as k8s_config
from kubernetes.client.rest import ApiException
from oauth2client.client import GoogleCredentials

import build_and_push_image
import util

TF_JOB_KIND = "TfJob"
TF_JOB_GROUP = "tensorflow.org"
TF_JOB_VERSION = "v1alpha1"
TF_JOB_PLURAL = "tfjobs"

# Template for a TfJob CRD YAML
TFJOB = '''apiVersion: "{{ tf_job_group }}/{{ tf_job_version }}"
kind: "{{ job_kind }}"
metadata:
  name: "{{ job_name }}"
  namespace: {{ namespace }}
spec:
  replicaSpecs:
    - replicas: 1
      tfReplicaType: MASTER
      template:
        spec:
          containers:
            - image: {{ image }}
              name: tensorflow
              args: {{ extra_args }}
          restartPolicy: OnFailure{% if num_workers > 0 %}
    - replicas: {{ num_workers }}
      tfReplicaType: WORKER
      template:
        spec:
          containers:
            - image: {{ image }}
              name: tensorflow
          restartPolicy: OnFailure{% endif %}{% if num_ps > 0 %}
    - replicas: {{ num_ps }}
      tfReplicaType: PS{% endif %}{% if deploy_tensorboard %}
  tensorBoard:
    logDir: {{ job_dir }}
{% endif %}
'''


def _current_gcloud_project():
  with open(os.devnull, 'w') as dev_null:
    return subprocess.check_output(['gcloud', 'config', 'list', 'project',
                                    '--format=value(core.project)'],
                                   stderr=dev_null).strip()


def _build_and_push_wrapper(model_build_path, skip_push=False):
  """Wrap build_and_push assuming registry, image label obtained auto. """

  # Get the dockerfile path and verify it is present
  dockerfile_path = os.path.join(model_build_path, 'Dockerfile')
  if not os.path.exists(dockerfile_path):
    dockerfile_path = os.path.join(model_build_path, 'Dockerfile.template')
    if not os.path.exists(dockerfile_path):
      raise ValueError('Could not find a Dockerfile or Dockerfile.template at '
                       'the specified docker build path.')

  image_label = os.path.split(model_build_path)[-1]

  image = 'gcr.io/%s/%s' % (_current_gcloud_project(),
                            image_label)

  base_images = {
      "cpu": "gcr.io/tensorflow/tensorflow:1.3.0",
      "gpu": "gcr.io/tensorflow/tensorflow:1.3.0-gpu",
  }

  images = build_and_push_image.build_and_push(dockerfile_path, image, modes=None,
                                               skip_push=skip_push, base_images=base_images)

  return images


def _get_api_client():
  k8s_config.load_kube_config()
  api_client = k8s_client.ApiClient()
  return api_client


def _generate_job_name(base_tag='tensorflow'):
  salt = datetime.datetime.now().strftime('%Y%m%d%H%M%S')
  job_name = base_tag + '-' + salt
  return job_name


def _create_job(api_client, args, write_job_config=False):

  crd_api = k8s_client.CustomObjectsApi(api_client)

  extra_args = ['--log_dir', args.job_dir]
  if args.extra_args is not None:
    # TODO: Probably want to do some checking
    extra_args.extend(args.extra_args.split(' '))

  # Could probably just pass kwargs?
  config = Environment().from_string(TFJOB).render(
      job_name=args.job_name, tf_job_group=TF_JOB_GROUP,
      tf_job_version=TF_JOB_VERSION,
      job_kind=TF_JOB_KIND, namespace=args.namespace,
      image=args.image, num_workers=args.num_workers,
      num_ps=args.num_ps, deploy_tensorboard=args.deploy_tensorboard,
      job_dir=args.job_dir, extra_args=extra_args)

  logging.info('Creating job with config:\n%s' % config)

  body_buffer = StringIO.StringIO(config)
  body = yaml.load(body_buffer)
  if args.use_gpu:
    body['spec']['replicaSpecs'][0]["template"]["spec"]["containers"][0]["resources"] = {
        "limits": {
            "nvidia.com/gpu": args.accelerator_count,
        }
    }

  if args.mock:
    logging.info("Running in --mock mode, skipping job submission.")
    return

  try:
    # Create a Resource
    api_response = crd_api.create_namespaced_custom_object(TF_JOB_GROUP,
                                                           TF_JOB_VERSION, args.namespace, TF_JOB_PLURAL, body)
    logging.info("Created job %s", api_response["metadata"]["name"])
  except ApiException as e:
    print("Exception when calling DefaultApi->apis_fqdn_v1_namespaces_namespace_resource_post: %s\n" %
          e)


def _poll_job(api_client, args):

  if args.mock:
    logging.info("Running in --mock mode, skipping job polling.")
    return

  crd_api = k8s_client.CustomObjectsApi(api_client)

  master_started = False
  runtime_id = None

  core_v1 = k8s_client.CoreV1Api(api_client)

  while True:

    results = crd_api.get_namespaced_custom_object(TF_JOB_GROUP,
                                                   TF_JOB_VERSION,
                                                   args.namespace,
                                                   TF_JOB_PLURAL,
                                                   args.job_name)

    if not runtime_id:
      runtime_id = results["spec"]["RuntimeId"]
      logging.info("Job has runtime id: %s", runtime_id)

      tensorboard_url = "http://127.0.0.1:8001/api/v1/proxy/namespaces/{namespace}/services/tensorboard-{runtime_id}:80/".format(
          namespace=args.namespace, runtime_id=runtime_id)
      logging.info("Tensorboard will be available at job\n %s",
                   tensorboard_url)

    if not master_started:
      # Get the master pod
      # TODO(jlewi): V1LabelSelector doesn't seem to help
      pods = core_v1.list_namespaced_pod(
          namespace=args.namespace, label_selector="runtime_id={0},job_type=MASTER".format(runtime_id))

    # TODO(jlewi): We should probably handle the case where more than 1 pod gets started.
    # TODO(jlewi): Once GKE logs pod labels we can just filter by labels to get all logs for a particular task
    # and not have to identify the actual pod.
    if pods.items:
      pod = pods.items[0]

      logging.info("master pod is %s", pod.metadata.name)
      query = {
          'advancedFilter': 'resource.type="container"\nresource.labels.namespace_id="default"\nresource.labels.pod_id="{0}"'.format(pod.metadata.name),
          'dateRangeStart': pod.metadata.creation_timestamp.isoformat(),
          'expandAll': 'false',
          'interval': 'NO_LIMIT',
          'logName': 'projects/{0}/logs/tensorflow'.format(args.project),
          'project': args.project,
      }
      logging.info("Logs will be available in stackdriver at\n"
                   "https://console.cloud.google.com/logs/viewer?" + urllib.urlencode(query))
      master_started = True

    if results["status"]["phase"] == "Done":
      break

    print("Job status {0}".format(results["status"]["phase"]))
    time.sleep(5)

  logging.info("Job %s", results["status"]["state"])


# Top level mode wrappers
def _run_deployment_mode(args):

  gke_api = discovery.build("container", "v1")

  cluster_request = {
      "cluster": {
          "name": args.cluster_name,
          "description": "A GKE cluster for TF.",
          "initialNodeCount": 1,
          "nodeConfig": {
              "machineType": args.machine_type,
              "oauthScopes": [
                  "https://www.googleapis.com/auth/cloud-platform"
              ],
          },
          # TODO(jlewi): Stop pinning GKE version once 1.8 becomes the default.
          "initialClusterVersion": "1.8.1-gke.1",
      }
  }

  if bool(args.accelerator) != (args.accelerator_count > 0):
    raise ValueError("If accelerator is set accelerator_count must be  > 0")

  if args.use_gpu:  # TODO: Remove this, for development when NVIDIA_K80_GPUS==0
    if args.accelerator:
      # TODO(jlewi): Stop enabling Alpha once GPUs make it out of Alpha
      cluster_request["cluster"]["enableKubernetesAlpha"] = True

      cluster_request["cluster"]["nodeConfig"]["accelerators"] = [
          {
              "acceleratorCount": args.accelerator_count,
              "acceleratorType": args.accelerator,
          },
      ]

  logging.info('Creating cluster from config:\n%s' %
               yaml.dump(cluster_request))
  util.create_cluster(gke=gke_api, project=args.project, zone=args.zone,
                      cluster_request=cluster_request)
  util.configure_kubectl(project=args.project, zone=args.zone,
                         cluster_name=args.cluster_name)

  api_client = _get_api_client()

  util.setup_cluster(api_client)


def _run_submit_mode(args):
  # Instantiate a Kubernetes API client object
  api_client = _get_api_client()

  if args.build_path is not None:
    logging.info('Building image from build path: %s' % args.build_path)
    if not os.path.exists(args.build_path):
      raise ValueError('The provided --build_path does not exist, '
                       'please provide a path to a directory containing '
                       'a Dockerfile.')
    images = _build_and_push_wrapper(args.build_path, skip_push=args.mock)
    if args.use_gpu:
      args.image = images['gpu']
    else:
      args.image = images['cpu']
  else:
    if not args.image:
      logging.error('If new image build is not being performed from '
                    '--build_path, the image to use for workers and '
                    'master nodes must be specified with --image.')
      exit(0)

  logging.info('Initiating training job with parameters:')
  logging.info('job_name: %s' % args.job_name)
  logging.info('image: %s' % args.image)

  # Create the training job
  _create_job(api_client, args, write_job_config=True)

  # # Poll the training job until completion, printing status
  _poll_job(api_client, args)


def _run_monitor_mode(args):
  # TODO: Ability to easily stream logs from a running job without going to
  # stackdriver?
  pass


if __name__ == "__main__":

  logging.getLogger().setLevel(logging.INFO)
  parser = argparse.ArgumentParser(
      description="Run a tensorflow training job on Kubernetes.")

  parser.add_argument(
      "--job_base_tag",
      default="tensorflow",
      type=str,
      help="The docker image to use.")

  parser.add_argument(
      "--job_name",
      default=None,
      type=str,
      help="Optionally, the exact job name to use.")

  parser.add_argument(
      "--build_path",
      default=None,
      type=str,
      help="Path to a directory containing a model image to be built.")

  parser.add_argument(
      "--image",
      default=None,
      type=str,
      help="The docker image to use for model instances.")

  parser.add_argument(
      '--num_workers',
      default=None,
      type=str,
      help='Number of tensorflow workers.')

  parser.add_argument(
      '--num_param_server',
      default=None,
      type=str,
      help='Number of tensorflow parameter servers.')

  parser.add_argument(
      '--use_gpu',
      dest="use_gpu",
      action="store_true",
      help='Whether to use GPUs on worker nodes.')

  parser.add_argument(
      '--project',
      default=None,
      type=str,
      help='GCloud project.')

  parser.add_argument(
      '--zone',
      default='us-east1-d',
      type=str,
      help='Compute zone.')  # Required for --accelerator=nvidia-tesla-k80?

  parser.add_argument(
      '--cluster_name',
      default='gke-tf-example',
      type=str,
      help='The name of the kubernetes cluster to create or access.')

  parser.add_argument(
      '--machine_type',
      default='n1-standard-8',
      type=str,
      help='Base cluster node instance type.')

  parser.add_argument(
      '--namespace',
      default='default',
      type=str,
      help='The Kubernetes namespace to use (must already exist).')
  # TODO: Give error when specifying namespace that doesn't already exist or
  #      create it.

  parser.add_argument(
      '--registry',
      default=None,
      type=str,
      help='GCloud project.')

  parser.add_argument(
      '--job_dir',
      default=None,
      type=str,
      help='Workdir on Google Cloud Storage to use for job artifacts.')

  parser.add_argument(
      '--deploy_tensorboard',
      default=True,
      type=bool,
      help='Whether to deploy tensorboard to monitor training jobs.')

  parser.add_argument(
      '--accelerator',
      default='nvidia-tesla-k80',
      type=str,
      help='Type of GPU accelerator.')

  parser.add_argument(
      '--accelerator_count',
      default="0",
      type=str,
      help='Number of accelerators to use.')

  parser.add_argument(
      '--num_ps',
      default="1",
      type=str,
      help='Number of parameter servers to use.')

  parser.add_argument(
      '--mode',
      default=None,
      type=str,
      help='Mode in which to run, one of ["deploy", "submit", "monitor"].')

  parser.add_argument(
      '--mock',
      default=False,
      action="store_true",
      help='Whether to mock the run mode.')

  parser.add_argument(
      '--extra_args',
      default=None,
      type=str,
      help='Extra arguments to add to the training task command.')

  parser.add_argument(
      '--deployment_mode',
      dest="deployment_mode",
      action="store_true",
      help='Whether to run in deployment mode.')
  # TODO: So might at this point be overloading what this one script does...

  args = parser.parse_args()

  if args.project is None:
    args.project = _current_gcloud_project()
    logging.info('current project: %s' % args.project)

  if args.registry is not None:
    args.registry = 'gcr.io/%s' % args.project
    logging.info('Container registry parameter not provided, assuming '
                 'gcr.io, i.e. %s' % args.registry)

  if not args.job_name:
    args.job_name = _generate_job_name(args.job_base_tag)

  if args.job_dir is None:
    args.job_dir = 'gs://%s-k8s/jobs/%s' % (args.project, args.job_name)
    # /%s' % (args.project, args.job_name)
    # args.job_dir = 'gs://%s-k8s/jobs' % args.project
    # _ensure_bucket(args.job_dir)

  if args.mode is None:
    raise ValueError('Please specify a run mode with --mode.')

  if args.mode == "deploy":
    _run_deployment_mode(args)
  elif args.mode == "submit":
    _run_submit_mode(args)
  elif args.mode == "monitor":
    _run_monitor_mode(args)
  else:
    raise ValueError('Unrecognized run --mode, saw: %s' % args.mode)
